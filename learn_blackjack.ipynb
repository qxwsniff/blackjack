{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\"> Learning to play Blackjack with Q-learning</div>\n",
    "\n",
    "In this notebook I will show how Q-learning can be used by an agent to teach itself to play Blackjack. I will then go on to compare its performance against a strategy recommended by a human expert.\n",
    "\n",
    "Our implementation is based on two key sources, which I highly recommend reading through:\n",
    "- Blackjack game adapted from:\n",
    "  https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\n",
    "- Q-learning algorithm adapted from:\n",
    "  https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0\n",
    "- Boltzmann policy adapted from:\n",
    "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf\n",
    "\n",
    "\n",
    "Contents:\n",
    "1. Overview of Blackjack environment \n",
    "2. Q-learning implementation and variations\n",
    "3. Connect Q-learning algorithm with environment\n",
    "4. Training for standard Q-learning\n",
    "5. Training for Double Q-learning\n",
    "6. Comparisons against human-strategy\n",
    "7. Discussion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Load modules & basic set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Overview of Blackjack environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created environment:\n",
      "Goal: 21 | DealerStop: 4 | DoubleBias: 0 | DoubleBoost: 2 | NumActions: 3\n"
     ]
    }
   ],
   "source": [
    "# load settings of environment from local file\n",
    "from settings import *\n",
    "\n",
    "# load the Blackjack environment from local file\n",
    "from blackjack_env import *\n",
    "\n",
    "# create environment\n",
    "env = BlackjackEnv()\n",
    "\n",
    "# Confirm the environment our agent will play in\n",
    "env_vals = (BLACKJACK_SCORE, DEALER_STOP, DOUBLE_BIAS, ODDS_BOOST, NUM_ACTIONS)\n",
    "print(\"Created environment:\")\n",
    "print(\"Goal: %i | DealerStop: %i | DoubleBias: %i | DoubleBoost: %i | NumActions: %i\" % env_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Q-learning implementation and variations\n",
    "\n",
    "We will start by implementing a series of selection policies, all of which control how our agent chooses actions. Since the agent will start with a blank canvas, we cannot simply let the agent always pick the \"best\" action. Rather, these policies help the agent manage the \"exploration-exploitation trade-off\". In all cases, the agent will start by prioritising \"exploration\": choosing actions mostly randomly so that it can experience as many different rewards and penalties as possible. As it gains more experience, it will begin to reduce the frequency of these random actions, instead prioritising \"exploitation\": using its experience so far to choose the action with highest expected future rewards.\n",
    "\n",
    "The policies implemented are:\n",
    "- e-greedy policy\n",
    "- boltzmann policy\n",
    "- both of the above but with Double-Q learning (see more in the following block of code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_egreedy(Q_row, number_actions, eps=0.9):\n",
    "    \"\"\"\n",
    "    Select an action (index) based on e-greedy policy,\n",
    "    e.g. random with probability e and per\n",
    "    current Q-values with probability 1-e\n",
    "    \"\"\"\n",
    "    if np.random.rand() < eps:\n",
    "        # pick by random choice\n",
    "        return np.random.choice(np.arange(0, number_actions))   \n",
    "    else:\n",
    "        # choose highest expected value action\n",
    "        return np.argmax(Q_row)    \n",
    "\n",
    "def select_boltzmann(Q_row, number_actions, tau=1):\n",
    "    \"\"\"\n",
    "    Select an action (index) based on current Q values and\n",
    "    the current state of annealing, e.g. using inverse\n",
    "    temperature, tau.\n",
    "    \n",
    "    PARAMS:\n",
    "    tau = inverse temperature, e.g. as tau approaches 0\n",
    "          boltzmann distribution approaches uniform sampling.\n",
    "          higher tau prefers actions with higher expected values\n",
    "    \"\"\"\n",
    "    # normalize wrt minimum values, ensuring no negatives\n",
    "    Q_row_ajd = Q_row - Q_row.min()\n",
    "\n",
    "    # calculate probabilities under boltzmann distribution\n",
    "    Q_pr = np.exp(tau * Q_row_ajd) / np.exp(tau * Q_row_ajd).sum()\n",
    "    \n",
    "    # Make selection (credit: Arthur Juliani's Medium article)\n",
    "    action_value = np.random.choice(Q_pr, p=Q_pr)\n",
    "    return np.argmax(Q_pr == action_value)\n",
    "\n",
    "def select_a(Q_row, policy=\"e\", param=0.9):\n",
    "    \"\"\" Select action from a row of Q for the given state\n",
    "    and for a given policy & parameter\n",
    "    \n",
    "    if policy == \"e\" then we will use e-greedy policy\n",
    "    if policy == \"b\" then we will use boltzmann distribution\n",
    "    \"\"\"\n",
    "    # choose action based on policy required\n",
    "    if policy==\"e\":\n",
    "        # use e-greedy policy, where param = epsilon\n",
    "        return select_egreedy(Q_row, env.action_space.n, param)\n",
    "    else:\n",
    "        # use boltzmann policy, where param = tau = inverse temperature\n",
    "        return select_boltzmann(Q_row, env.action_space.n, param)\n",
    "    \n",
    "def doubleQ_select(Q_a, Q_b, s, policy=\"e\", policy_param = 0.9):\n",
    "    \"\"\"\n",
    "    Function which enables selection using two Q matrices. Per van Hasselt's\n",
    "    original implementation:\n",
    "       'In our experiments, we calculated the average of the two Q values for each\n",
    "    action and then performed e-greedy exploration with the resulting average\n",
    "    Q values.' (Hado van Hasselt)\n",
    "    \n",
    "    Optional params:\n",
    "        policy = \"e\" or \"b\" for e-greedy and boltzmann respectively\n",
    "        policy_param = float, epsilon if e-greedy or inverse temperature (tau)\n",
    "                       if boltzmann \n",
    "    \"\"\"\n",
    "    \n",
    "    # take average Q values under consideration for given state\n",
    "    Q_avg_row =  (Q_a[s] + Q_b[s])/2\n",
    "    \n",
    "    # choose action based on given policy and policy parameter\n",
    "    return select_a(Q_avg_row, policy, policy_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move to defining our update rules, which is how the agent learns from experience in playing Blackjack. We will use the standard update rule as well as a variation called **Double Q learning**, where two matrixes are maintained to avoid over-estimation of expected returns. For more information on this variation see https://arxiv.org/abs/1509.06461 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanillaQ_update(Q, r, s, s1, a, lr, y):\n",
    "    \"\"\"\n",
    "    Update Q-matrix using standard update rule\n",
    "    \"\"\"\n",
    "    Q[s][a] = Q[s][a] + lr*(r + y*np.max(Q[s1]) - Q[s][a])\n",
    "    return Q\n",
    "\n",
    "def doubleQ_update(Q_a, Q_b, r, s0, s1, a,\n",
    "                   lr, y, prob_update_Q_a = 0.5):\n",
    "    \"\"\"\n",
    "    Randomly select a Q matrix to update, estimating future\n",
    "    rewards from the other matrix. Per van Hasselt's original\n",
    "    implementation.\n",
    "    \n",
    "    Parameters:\n",
    "        - prob_update_Q_a: probability of updating the first matrix.\n",
    "                           (allows experimentation with reliance on\n",
    "                           the first Q matrix vs the second Q matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    if np.random.rand() < prob_update_Q_a:\n",
    "        # update Q_a matrix, taking copy before update for reference\n",
    "        Q_old = np.copy(Q_a)\n",
    "        optimal_a = np.argmax(Q_a[s1][a])\n",
    "        Q_a[s0][a] = Q_a[s0][a] + lr*(r + y*Q_b[s1][optimal_a] - Q_a[s0][a])\n",
    "        \n",
    "    else:\n",
    "        # update Q_b matrix, taking copy before update for reference\n",
    "        Q_old = np.copy(Q_b)\n",
    "        optimal_b = np.argmax(Q_b[s1][a])\n",
    "        Q_b[s0][a] = Q_b[s0][a] + lr*(r + y*Q_a[s1][optimal_b] - Q_b[s0][a])\n",
    "        \n",
    "    return Q_a, Q_b, Q_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Connect Q-learning algorithm with environment\n",
    "\n",
    "Now that we have an environment and functions that implement Q-learning, we can connect them together and watch our agent learn to play! We will put this functionality into a function so that we can easily recall it under different parameters in the rest of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playGame(Q_a=None, Q_b = None, verbose=0,\n",
    "             dbl_Q=0, lr=0.1, y=0.6,\n",
    "             policy = \"e\", policy_param=0.7,\n",
    "             num_episodes = 1, cb_over_num_eps=10000):\n",
    "    \"\"\"\n",
    "    Main function where agent learns to play Blackjack.\n",
    "    \n",
    "    Inputs:\n",
    "      Q_a, Q_b        = None, None or pre-trained matrix, None\n",
    "      verbose         = verbosity level\n",
    "      dbl_Q           = 1 if using double Q learning\n",
    "      policy          = \"e\" for e-greedy, \"b\" for boltzmann\n",
    "      policy_param    = float controlling above policy\n",
    "      num_episodes    = number of games to play\n",
    "      cb_over_num_eps = maintain a callback version of agent \n",
    "                        if they have superior performance over\n",
    "                        this number of games\n",
    "                        \n",
    "    Outputs:\n",
    "      Q_out           = Q-matrix of agent after final episode\n",
    "      rList           = list of agent's rewards in each episode \n",
    "      Q_cb            = Q-matrix of callback agent\n",
    "      last_cb_ep      = episode number where callback agent was created\n",
    "    \"\"\"\n",
    "    \n",
    "    # state space needs to be blackjack_score+11\n",
    "    # e.g. ace on top of a score of 21\n",
    "    if Q_a is None:\n",
    "        Q_a = np.zeros([BLACKJACK_SCORE+11,11,2,env.action_space.n])\n",
    "\n",
    "    # take a copy of Q matrix if we are using double Q learning\n",
    "    if dbl_Q != 0:\n",
    "        Q_b = np.copy(Q_a)\n",
    "    \n",
    "    # prepare a 'call-back' version of the Q-matrix\n",
    "    # e.g. the one with best mean rewards over last N episodes\n",
    "    Q_cb = Q_a.copy()\n",
    "    best_meanR = -999999    # a very low bar to beat, e.g. so that a Q-matrix of 0s doesn't beat this!\n",
    "    last_cb_ep = 0          # to record the last episode the call-back is made to\n",
    "    a_freq = np.zeros(env.action_space.n)\n",
    "\n",
    "    # if using Boltzmann policy, set rate of annealing (tau = inverse temperature)\n",
    "    if policy==\"b\":\n",
    "        tau_growth = (200/policy_param)**(1/num_episodes)\n",
    "\n",
    "    #create lists to contain total rewards and steps per episode\n",
    "    rList = []\n",
    "    aList = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        rAll = 0\n",
    "        d = False\n",
    "        j = 0\n",
    "        \n",
    "        # game will not exceed 20 moves, e.g. simple while loop with a ceiling out of caution\n",
    "        while j < 20:\n",
    "            j+=1\n",
    "\n",
    "            # choose an acion per given policy        \n",
    "            if dbl_Q==0:\n",
    "                Q_row = Q_a[s]\n",
    "                a = select_a(Q_row, policy, policy_param)  \n",
    "            else:\n",
    "                Q_avg_row =  (Q_a[s] + Q_b[s])/2\n",
    "                a = select_a(Q_avg_row, policy, policy_param) \n",
    "                \n",
    "            # log the actions chosen\n",
    "            a_freq[a] += 1\n",
    "            aList.append(a)\n",
    "\n",
    "            #Get new state and reward from environment\n",
    "            Q_old = np.copy(Q_a)\n",
    "            player_hand_old = env.player.copy()\n",
    "            s1,r,d,_ = env.step(a)\n",
    "            \n",
    "            #Update Q-Table with new knowledge, unless we are in a testing scenario where lrate == 0.0\n",
    "            if lr > 0:\n",
    "                if dbl_Q==0:\n",
    "                    Q_a[s][a] = Q_a[s][a] + lr*(r + y*np.max(Q_a[s1]) - Q_a[s][a])\n",
    "                else:\n",
    "                    # Double-Q learning: use the second Q-matrix as estimate of expected returns\n",
    "                    Q_a, Q_b, Q_old = doubleQ_update(Q_a, Q_b, r, s, s1, a, lr, y)\n",
    "    \n",
    "            # inspect the step & Q matrix once, at 1/5 through training\n",
    "            if (i == num_episodes//5) & (verbose > 2):\n",
    "                if ((dbl_Q==0)|(np.array_equal(Q_b, Q_old))):\n",
    "                    Q_new = np.copy(Q_a)\n",
    "                else:\n",
    "                    Q_new = np.copy(Q_b) \n",
    "                show_step(player_hand_old, env.player, env.dealer, s, s1, a, Q_old, Q_new, r, d)\n",
    "            \n",
    "            # update rewards and state\n",
    "            rAll += r\n",
    "            s = s1\n",
    "\n",
    "            if d == True:            \n",
    "                #Reduce chance of random action as we train the model.\n",
    "                # for e-greedy policy, reduce epsilon\n",
    "                if policy==\"e\":\n",
    "                    if policy_param >= 0.5:\n",
    "                        policy_param *= 0.99999\n",
    "                    else:\n",
    "                        policy_param *= 0.9999\n",
    "                        \n",
    "                # for boltzmann policy, increase tau (inverse temperature)\n",
    "                if policy==\"b\":\n",
    "                    policy_param *= tau_growth\n",
    "                \n",
    "                # For each of the final 5 episodes, show the game for inspection\n",
    "                if (i >= (num_episodes-5)) & (verbose > 1):\n",
    "                    show_ep(env.player, env.dealer, r, i)\n",
    "                \n",
    "                break\n",
    "\n",
    "        # append reward to list\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        # update the callback Q-matrix IIF better average over last N episodes\n",
    "        meanR_lastN_eps = meanR_lastN(rList, n=cb_over_num_eps)\n",
    "        if ((i > cb_over_num_eps) & (meanR_lastN_eps > best_meanR)):\n",
    "            \n",
    "            # take average of Q-values first if double Q learning\n",
    "            if dbl_Q == 0:\n",
    "                Q_cb = np.copy(Q_a)\n",
    "            else:\n",
    "                Q_cb = (Q_a + Q_b) / 2\n",
    "            last_cb_ep = i\n",
    "            best_meanR = meanR_lastN_eps\n",
    "            \n",
    "        # show some training feedback every n episodes\n",
    "        feedback_freq = 25000\n",
    "        if verbose > 0:\n",
    "            if i % feedback_freq == 0:\n",
    "                print(\"Ep: {}\\tMean reward over last {} episodes: {}. ({}-policy @ {})\".format(i,\n",
    "                        cb_over_num_eps, meanR_lastN_eps,\n",
    "                        policy, np.around(policy_param, decimals=5)))\n",
    "        \n",
    "    # show overall success history of agent \n",
    "    if verbose > 1:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(np.cumsum(rList))\n",
    "        plt.title('Performance vs. episodes')\n",
    "        plt.ylabel('Total Returns')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.axvline(x=last_cb_ep, linestyle=\"--\", color=\"black\")    # show the last callback episode\n",
    "        plt.show()\n",
    "        \n",
    "        pc_actions_taken = a_freq/a_freq.sum()\n",
    "        print(pc_actions_taken)\n",
    "        \n",
    "    # before returning Q matrix, take average if double Q-learning\n",
    "    if dbl_Q == 0:\n",
    "        Q_out = np.copy(Q_a)\n",
    "    else:\n",
    "        Q_out = (Q_a + Q_b)/2\n",
    "        \n",
    "    return Q_out, rList, Q_cb, last_cb_ep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Training for standard Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we want to have some functions that let us evaluate the agent's performance numerically, as well as some ability to visually inspect what is going on at both a high- and granular-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(l):\n",
    "    \"\"\" Find the mode of a given list l. \"\"\"\n",
    "    return max(set(l), key=l.count)\n",
    "\n",
    "def meanR_lastN(r_list, n=200):\n",
    "    \"\"\" Find mean over the last n items in a list \"\"\"\n",
    "    return np.mean(r_list[-n:])\n",
    "\n",
    "def action_dict(a, short=False):\n",
    "    if short:\n",
    "        lookup = {0: \"St\", 1: \"Hi\", 2: \"Db\", 3: \"Su\", 4: \"In\"}\n",
    "    else:\n",
    "        lookup = {0: \"Stick\", 1: \"Hit\", 2: \"Double\", 3: \"Surrender\", 4: \"Invest\"}\n",
    "    return lookup[a]\n",
    "\n",
    "def convert_first_1_to_11(l):\n",
    "    \"\"\" Convert the first 1 in a hand to 11, e.g. to faciliate understanding of hands played involving an Ace \"\"\"\n",
    "    for idx, i in enumerate(l):\n",
    "        if i == 1:\n",
    "            l[idx] = 11\n",
    "            break\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_ep(player_hand, dealer_hand, reward, ep_no, cumulative=True):\n",
    "    \"\"\" Summarise the episode once it is complete \"\"\"\n",
    "    pl_bust = \"\"\n",
    "    dl_bust = \"\"\n",
    "    \n",
    "    # show if anyone went bust\n",
    "    if is_bust(player_hand):\n",
    "        pl_bust = \"XXX\"\n",
    "    if is_bust(dealer_hand):\n",
    "        dl_bust = \"XXX\"\n",
    "    \n",
    "    # show who won\n",
    "    if reward > 0:\n",
    "        pl_win = \"!!!\"\n",
    "        dl_win = \"\"\n",
    "    elif reward < 0:\n",
    "        pl_win = \"\"\n",
    "        dl_win = \"!!!\"\n",
    "    else:\n",
    "        pl_win = \"---\"\n",
    "        dl_win = \"---\"\n",
    "        \n",
    "    # update hands to show effects of an Ace being used as 11 (instead of 1)\n",
    "    if usable_ace(player_hand):\n",
    "        player_hand = convert_first_1_to_11(player_hand)\n",
    "    if usable_ace(dealer_hand):\n",
    "        dealer_hand = convert_first_1_to_11(dealer_hand)\n",
    "        \n",
    "    # display results\n",
    "    print(\"--- SUMMARY OF EP #{} ---\".format(ep_no))\n",
    "    if cumulative:\n",
    "        # simplest view\n",
    "        print(\"Player:\\t{} {}{}\".format(np.cumsum(player_hand), pl_bust, pl_win))\n",
    "        print(\"Dealer:\\t{} {}{}\".format(np.cumsum(dealer_hand), dl_bust, dl_win))\n",
    "        print(player_hand)\n",
    "        print(dealer_hand)\n",
    "    else:\n",
    "        # more detailed view, showing progression of cards from initial (visible) hands to final hands\n",
    "        print(\"Player:\\t{} >> {} >> {} {}{}\".format(player_hand[:2], player_hand, sum_hand(player_hand),\n",
    "                                                    pl_bust, pl_win))\n",
    "        print(\"Dealer:\\t{} >> {} >> {} {}{}\".format(dealer_hand[:1], dealer_hand, sum_hand(dealer_hand),\n",
    "                                                    dl_bust, dl_win))\n",
    "\n",
    "def show_step(player_hand_old, player_hand_new, dealer_hand,\n",
    "              s0, s1, a, Q0, Q1, r, d):\n",
    "    \"\"\" Give a detailed view of each step in a hand \"\"\"\n",
    "    # current state summary and action\n",
    "    print(\"\"\"-----------------------\n",
    "Player's old hand:\\t{} (={})\n",
    "Dealer's face-up card:\\t{}\n",
    "Player had useable:\\t{}\n",
    "Player action:\\t\\t{} ({})\"\"\".format(player_hand_old, s0[0], s0[1], s0[2], a, action_dict(a)))\n",
    "    \n",
    "    # effects of action\n",
    "    if player_hand_old != player_hand_new:\n",
    "        print(\"\"\"Player got card:\\t{}\"\"\".format(player_hand_new[-1]))\n",
    "        \n",
    "    # Dealer's eventual outcome\n",
    "    print(\"Dealer's hand:\\t{} (={})\".format(dealer_hand, sum_hand(dealer_hand)))\n",
    "    print(\"{}\\n{}\".format(s0, s1))\n",
    "    \n",
    "    print(\"r:{} | Done: {}\".format(r, d))\n",
    "    \n",
    "    # show two Q matrices separately\n",
    "    inspect_Q(Q0)\n",
    "    inspect_Q(Q1)\n",
    "    \n",
    "    # show the difference\n",
    "    Q_diff = Q1 - Q0\n",
    "    inspect_Q(Q_diff)\n",
    "\n",
    "def inspect_Q(Q_in):\n",
    "    \"\"\" Inspect the Q matrix as heatmaps\n",
    "    Split out each action's Q-values for all possible states\"\"\"\n",
    "        \n",
    "    # Heatmaps: Show side-by-side action Q-matrix, with two rows for each Usable Ace state\n",
    "    color_limits = np.max([abs(np.min(Q_in)), np.max(Q_in)])\n",
    "    i_labels = ['No useable ace', 'Useable ace']\n",
    "    j_labels = ['Stick', 'Hit', 'Double', 'Surrender', 'Invest'][:NUM_ACTIONS]\n",
    "    \n",
    "    # new row of plots for each of the binary states Ace / no Ace\n",
    "    for i in np.arange(0, len(i_labels)):    \n",
    "        plt.figure()\n",
    "        f, axes = plt.subplots(1,len(j_labels), sharex=False, sharey=False,\n",
    "                                          figsize=(12,6))\n",
    "        plt.title(i_labels[i])\n",
    "        # show Q values for each possible action, across all hand values x initial dealer card\n",
    "        for j in np.arange(0, len(j_labels)):\n",
    "            # show each heat map, adding color bar for the last plot only\n",
    "            showCbar = j == len(j_labels)-1\n",
    "            # obtain the relevant Q values - hiding non-used states (4- & 22+ for player / 0 for dealer)\n",
    "            g = sns.heatmap(Q_in[4:22,1:,i,j], cmap='seismic_r', cbar=showCbar,\n",
    "                            vmin = -color_limits, vmax = color_limits, center=0, ax=axes[j])\n",
    "            g.set_title(\"{}\\n - {} - \".format(j_labels[j], i_labels[i]))\n",
    "            g.set_xlabel('Dealer initial card')\n",
    "            # set agent hand value range of 4-21 inclusive, dealer 2-11 inclusive\n",
    "            g.set_yticklabels(np.arange(4,22), rotation=0)\n",
    "            g.set_xticklabels(np.arange(1,11), rotation=0)  # where Ace = 1 for now\n",
    "            # only show y label on first plot\n",
    "            if j == 0:\n",
    "                g.set_ylabel('Agent hand value')\n",
    "                \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to start training. All we have to do first is choose how long we want to train the agent for, how many games we are going to evaluate it over, and of course some hyper-parameter values. We will start with some which seem sensible, and move onto more robust experimentation in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scope of training and evaluation\n",
    "NUM_EPISODES_TRAIN = 500000                # number of episodes to train agent over\n",
    "EVAL_LAST_N_EPS = 50000                    # evaluate agent with mean rewards over last (N) episodes  \n",
    "NUM_TO_TEST = int(EVAL_LAST_N_EPS / 50)    # number of episodes to test agent over per round (learning OFF)\n",
    "NUM_TEST_ROUNDS = 1000                     # number of rounds N (each of Z episodes) to test agent(s) over\n",
    "\n",
    "### Set verbosity level\n",
    "# 0: nothing\n",
    "# 1: validation metrics only\n",
    "# 2: validation, perf-vs-episodes, last 5 games\n",
    "# 3: inspect Q-matrix update 1/5 of way into training\n",
    "VERBOSE =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyper-parameters: grid format will be beneficial when we optimize these \n",
    "grid = ParameterGrid({\"dbl_Q\" : [0],                # 1 to use Double Q learning\n",
    "                          \"lr\": [0.01],             # learning rate\n",
    "                          \"y\": [0.3],               # gamma\n",
    "                          \"policy\": \"b\",            # \"e\" for e-greedy, \"b\" for boltzmann\n",
    "                          \"policy_param\":  [1],     # epsilon / tau (inverse temperature)\n",
    "                          \"num_episodes\": [NUM_EPISODES_TRAIN],\n",
    "                          \"cb_over_num_eps\": [EVAL_LAST_N_EPS],\n",
    "                          \"verbose\": [VERBOSE]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training an agent in the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Now training agent 1/1\n",
      "DblQ freq: 0 | LR: 0.01 | g: 0.3 | pol: b | pol_param: 1\n",
      "Ep: 0\tMean reward over last 50000 episodes: -1.0. (b-policy @ 1.00001)\n",
      "Ep: 25000\tMean reward over last 50000 episodes: -0.47906083756649737. (b-policy @ 1.30334)\n",
      "Ep: 50000\tMean reward over last 50000 episodes: -0.4171. (b-policy @ 1.69866)\n"
     ]
    }
   ],
   "source": [
    "# set up evaluation metric arrays\n",
    "eval_metrics = []\n",
    "\n",
    "# iterate through the grid search    \n",
    "grid_size = len(grid)\n",
    "for p_idx, params in enumerate(grid):\n",
    "    print(\"---------------------------\\nNow training agent {}/{}\".format(p_idx+1, grid_size))\n",
    "    print(\"DblQ freq: {} | LR: {} | g: {} | pol: {} | pol_param: {}\".format(params['dbl_Q'], params['lr'],\n",
    "                                                          params['y'], params['policy'], np.round_(params['policy_param'], decimals=5)))\n",
    "\n",
    "    # train agent using current parameters\n",
    "    Q, rList, Q_CB, lastCB_ep = playGame(Q_a=None, Q_b=None, **params)\n",
    "\n",
    "    # calculate evaluation metrics\n",
    "    eval_final = meanR_lastN(rList, n=EVAL_LAST_N_EPS)\n",
    "    eval_cb = meanR_lastN(rList[:lastCB_ep], n=EVAL_LAST_N_EPS)\n",
    "\n",
    "    if VERBOSE>0:\n",
    "        print(\"---------------------------\\nValidation metrics\\n---------------------------\")\n",
    "        print(\"After final episode, agent achieved mean reward of {} over last {} episodes\".format(eval_final, EVAL_LAST_N_EPS))\n",
    "        print(\"After episode {}, agent achieved mean reward of {} over last {} episodes\".format(lastCB_ep, eval_cb, EVAL_LAST_N_EPS))\n",
    "\n",
    "    # Pickle Q matrices for later user\n",
    "    exportMe = False\n",
    "    if exportMe:\n",
    "        print(\"---------------------------\\nPickling agents\")\n",
    "        export_name = 'Qmatrix_expID_{}_Fi_DblQ{}_lr{}_g{}_pol{}_polparam{}.p'.format(p_idx, params['dbl_Q'], params['lr'],\n",
    "                                                                    params['y'], params['policy'], params['policy_param'])\n",
    "        name_fi = Q_DATA_DIR + export_name\n",
    "        pickle.dump(Q , open(name_fi, \"wb\" ) )\n",
    "\n",
    "        # Pickle the call back too, referencing the call-back episode in filename\n",
    "        export_name_cb = 'Qmatrix_expID_{}_CB{}_DblQ{}_lr{}_g{}_pol{}_polparam{}.p'.format(p_idx, lastCB_ep,\n",
    "                                                                                  params['dbl_Q'], params['lr'],\n",
    "                                                                                  params['y'], params['policy'], params['policy_param'])\n",
    "        name_cb = Q_DATA_DIR + export_name_cb \n",
    "        pickle.dump(Q_CB, open(name_cb, \"wb\" ) )\n",
    "\n",
    "        # pickle the rLists using for later analysis (same filenames but different folder)\n",
    "        pickle.dump(rList , open(RLIST_DATA_DIR + export_name, \"wb\" ) )\n",
    "        pickle.dump(rList[:lastCB_ep], open(RLIST_DATA_DIR + export_name_cb, \"wb\" ) )\n",
    "\n",
    "\n",
    "    # form Vector for validation metrics, for both final agent and call-back agent\n",
    "    eval_vec_fi = [name_fi, params['dbl_Q'], params['lr'], params['y'], params['policy'], params['policy_param'],\n",
    "                   params['num_episodes'], params['cb_over_num_eps'], 0, eval_final]\n",
    "    eval_vec_cb = [name_cb, params['dbl_Q'], params['lr'], params['y'], params['policy'], params['policy_param'],\n",
    "                   params['num_episodes'], params['cb_over_num_eps'], lastCB_ep, eval_cb]\n",
    "    eval_metrics.append(eval_vec_fi)\n",
    "    eval_metrics.append(eval_vec_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gym_env)",
   "language": "python",
   "name": "gym_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
